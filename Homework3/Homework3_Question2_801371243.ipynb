{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6f47bcc-d64d-4d81-926a-538d2f15d6e5",
   "metadata": {},
   "source": [
    "Build the model for.LSTM and rnn.GRU for the tiny Shakespeare dataset, the data loader code is already provided.\r\n",
    "\r\n",
    "Train the models for the sequence of 20 and 30, report and compare training loss, validation accuracy, execution time for training, and computational and mode size complexities across the two models.\r\n",
    "Adjust the hyperparameters (fully connected network, number of hidden layers, and the number of hidden states) and compare your results (training and validation loss, computation complexity, model size, training and inference time, and the output sequence). Analyze their influence on accuracy, running time, and computational perplexity.\r\n",
    "What if we increase the sequence length to 50? Perform the training and report the accuracy and model complexity results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a0d1324-8e5c-4a93-aaec-74bcc5afc6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Download the dataset\n",
    "url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "# Prepare the dataset\n",
    "sequence_length = 20\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "# Create sequences and targets\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Define the dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "dataset = CharDataset(sequences, targets)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "\n",
    "# Define the LSTM model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.lstm(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "# Define the GRU model\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, _ = self.gru(embedded)\n",
    "        output = self.fc(output[:, -1, :])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67b5f24a-b5b2-4b54-a34e-11aea0c133c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Number of trainable parameters: 559681\n",
      "Epoch 1/10, Train Loss: 1.7076634001574569, Test Loss: 1.5441576732141973, Test Accuracy: 53.19107923344167%, Inference Time: 10.234908103942871 seconds\n",
      "Epoch 2/10, Train Loss: 1.4844536945283653, Test Loss: 1.4690568940227768, Test Accuracy: 55.16485486943853%, Inference Time: 9.065066814422607 seconds\n",
      "Epoch 3/10, Train Loss: 1.424019436503168, Test Loss: 1.430919986994049, Test Accuracy: 55.891964585901604%, Inference Time: 9.149778604507446 seconds\n",
      "Epoch 4/10, Train Loss: 1.3885324373371082, Test Loss: 1.4122446166053286, Test Accuracy: 56.7091785274011%, Inference Time: 8.964710235595703 seconds\n",
      "Epoch 5/10, Train Loss: 1.3638709488950511, Test Loss: 1.397402367780623, Test Accuracy: 57.01714669954051%, Inference Time: 8.97447919845581 seconds\n",
      "Epoch 6/10, Train Loss: 1.344998960326064, Test Loss: 1.3912321806501124, Test Accuracy: 56.99114647540065%, Inference Time: 8.885128498077393 seconds\n",
      "Epoch 7/10, Train Loss: 1.3298790680561952, Test Loss: 1.3796298053848695, Test Accuracy: 57.521013112182004%, Inference Time: 8.898829460144043 seconds\n",
      "Epoch 8/10, Train Loss: 1.3175192407609673, Test Loss: 1.3752818428026967, Test Accuracy: 57.711980275692035%, Inference Time: 9.051382303237915 seconds\n",
      "Epoch 9/10, Train Loss: 1.3058777209745354, Test Loss: 1.374359668942614, Test Accuracy: 57.540289140423624%, Inference Time: 8.778393983840942 seconds\n",
      "Epoch 10/10, Train Loss: 1.297126750536028, Test Loss: 1.3701181626839674, Test Accuracy: 57.68418693264597%, Inference Time: 8.8803231716156 seconds\n",
      "Total Training Time: 1179.0623676776886 seconds\n",
      "Model Size: 776 bytes\n",
      "\n",
      "Training GRU model...\n",
      "Number of trainable parameters: 428097\n",
      "Epoch 1/10, Train Loss: 1.6938343098535664, Test Loss: 1.5467014913920076, Test Accuracy: 53.131009750084054%, Inference Time: 6.365835189819336 seconds\n",
      "Epoch 2/10, Train Loss: 1.4998685097085698, Test Loss: 1.489887813556078, Test Accuracy: 54.71657514288916%, Inference Time: 6.2949745655059814 seconds\n",
      "Epoch 3/10, Train Loss: 1.453840221121813, Test Loss: 1.4683546737494717, Test Accuracy: 55.308752661660876%, Inference Time: 19.507712602615356 seconds\n",
      "Epoch 4/10, Train Loss: 1.4275584328519977, Test Loss: 1.447204621920145, Test Accuracy: 55.76330830438194%, Inference Time: 3.7561097145080566 seconds\n",
      "Epoch 5/10, Train Loss: 1.4101873627525598, Test Loss: 1.4399448386294638, Test Accuracy: 56.11251821136389%, Inference Time: 3.789942741394043 seconds\n",
      "Epoch 6/10, Train Loss: 1.39794021323366, Test Loss: 1.4353257516848925, Test Accuracy: 55.886136949456464%, Inference Time: 3.4059457778930664 seconds\n",
      "Epoch 7/10, Train Loss: 1.3894329198598179, Test Loss: 1.431867844608414, Test Accuracy: 56.300795696514626%, Inference Time: 3.486192226409912 seconds\n",
      "Epoch 8/10, Train Loss: 1.3832358807342462, Test Loss: 1.4232071557403363, Test Accuracy: 56.39941723635548%, Inference Time: 3.610246181488037 seconds\n",
      "Epoch 9/10, Train Loss: 1.3781950799251383, Test Loss: 1.422371319635668, Test Accuracy: 56.28645074526504%, Inference Time: 3.661344528198242 seconds\n",
      "Epoch 10/10, Train Loss: 1.3741431738016459, Test Loss: 1.4180793850434217, Test Accuracy: 56.42810713885464%, Inference Time: 3.535126209259033 seconds\n",
      "Total Training Time: 633.2908935546875 seconds\n",
      "Model Size: 776 bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Train the model with validation loss, model size, number of parameters, and inference time\n",
    "def train_model(model, train_loader, test_loader, device, num_epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Calculate the number of trainable parameters\n",
    "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'Number of trainable parameters: {num_params}')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        test_loss = 0\n",
    "        inference_start_time = time.time()  # Start timer for inference\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                test_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        inference_end_time = time.time()  # End timer for inference\n",
    "        inference_time = inference_end_time - inference_start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader)}, Test Loss: {test_loss/len(test_loader)}, Test Accuracy: {100 * correct / total}%, Inference Time: {inference_time} seconds')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'Total Training Time: {end_time - start_time} seconds')\n",
    "\n",
    "    # Calculate model size\n",
    "    model_size = sys.getsizeof(model.state_dict())\n",
    "    print(f'Model Size: {model_size} bytes')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize and train the LSTM model\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "train_model(lstm_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "print(\"\\nTraining GRU model...\")\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "train_model(gru_model, train_loader, test_loader, device, num_epochs, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5bdffa4d-476a-45af-98f7-a123aa4c7f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Number of trainable parameters: 148801\n",
      "Epoch 1/10, Train Loss: 1.8319664558014237, Test Loss: 1.7497204281582068, Test Accuracy: 48.02465538496021%, Inference Time: 3.2053937911987305 seconds\n",
      "Epoch 2/10, Train Loss: 1.736948809077257, Test Loss: 1.7414866521723709, Test Accuracy: 48.492211139751205%, Inference Time: 3.190444231033325 seconds\n",
      "Epoch 3/10, Train Loss: 1.7324021682271449, Test Loss: 1.737878772491569, Test Accuracy: 48.50521125182114%, Inference Time: 3.4379940032958984 seconds\n",
      "Epoch 4/10, Train Loss: 1.7348074167368401, Test Loss: 1.7469942238025808, Test Accuracy: 48.314244088311106%, Inference Time: 3.2713723182678223 seconds\n",
      "Epoch 5/10, Train Loss: 1.7375169511813922, Test Loss: 1.7604003551015892, Test Accuracy: 47.85475736859801%, Inference Time: 3.1926138401031494 seconds\n",
      "Epoch 6/10, Train Loss: 1.7477148259705124, Test Loss: 1.7704925211557903, Test Accuracy: 47.39840860697075%, Inference Time: 3.3794829845428467 seconds\n",
      "Epoch 7/10, Train Loss: 1.7579351154231369, Test Loss: 1.7666931938865167, Test Accuracy: 47.606858679816206%, Inference Time: 3.378282308578491 seconds\n",
      "Epoch 8/10, Train Loss: 1.7626042570694382, Test Loss: 1.7707651415845134, Test Accuracy: 47.98475848929732%, Inference Time: 3.295412540435791 seconds\n",
      "Epoch 9/10, Train Loss: 1.7760007909434457, Test Loss: 1.791592029409031, Test Accuracy: 47.27916619970862%, Inference Time: 3.1623663902282715 seconds\n",
      "Epoch 10/10, Train Loss: 1.79464846384368, Test Loss: 1.80892672562011, Test Accuracy: 46.56012551832343%, Inference Time: 3.2852694988250732 seconds\n",
      "Total Training Time: 465.6598768234253 seconds\n",
      "Model Size: 776 bytes\n",
      "\n",
      "Training GRU model...\n",
      "Number of trainable parameters: 115777\n",
      "Epoch 1/10, Train Loss: 2.0545456038934367, Test Loss: 2.0329588713210955, Test Accuracy: 42.132914938921886%, Inference Time: 3.091066360473633 seconds\n",
      "Epoch 2/10, Train Loss: 2.0323425143598075, Test Loss: 2.010708606660332, Test Accuracy: 42.01322425193321%, Inference Time: 3.0845274925231934 seconds\n",
      "Epoch 3/10, Train Loss: 2.0285686530809737, Test Loss: 2.067571840406627, Test Accuracy: 40.23086405917292%, Inference Time: 3.053173065185547 seconds\n",
      "Epoch 4/10, Train Loss: 2.0415884026048925, Test Loss: 2.0498746010467914, Test Accuracy: 40.324106242295194%, Inference Time: 3.1474075317382812 seconds\n",
      "Epoch 5/10, Train Loss: 2.0493735459708513, Test Loss: 2.060971226900115, Test Accuracy: 41.39459822929508%, Inference Time: 3.153717517852783 seconds\n",
      "Epoch 6/10, Train Loss: 2.043375555000562, Test Loss: 2.0244699199515654, Test Accuracy: 42.09525944189174%, Inference Time: 3.0694735050201416 seconds\n",
      "Epoch 7/10, Train Loss: 2.057305373802858, Test Loss: 2.0956779738233067, Test Accuracy: 39.825170906645745%, Inference Time: 3.4148190021514893 seconds\n",
      "Epoch 8/10, Train Loss: 2.0602297184799707, Test Loss: 2.102448491549533, Test Accuracy: 38.400537935671856%, Inference Time: 3.0736708641052246 seconds\n",
      "Epoch 9/10, Train Loss: 2.0605050627349506, Test Loss: 2.0608582556145123, Test Accuracy: 40.95752549590945%, Inference Time: 3.0441455841064453 seconds\n",
      "Epoch 10/10, Train Loss: 2.057537422208217, Test Loss: 2.053096058378258, Test Accuracy: 41.945085733497706%, Inference Time: 3.0553860664367676 seconds\n",
      "Total Training Time: 466.3857741355896 seconds\n",
      "Model Size: 776 bytes\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the LSTM model\n",
    "input_size = len(chars)\n",
    "hidden_size = 128\n",
    "output_size = len(chars)\n",
    "num_epochs = 10\n",
    "lr = 0.01\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "train_model(lstm_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "print(\"\\nTraining GRU model...\")\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "train_model(gru_model, train_loader, test_loader, device, num_epochs, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7228500e-a20b-487e-9d0f-474cf62d84d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset with sequence length 30\n",
    "sequence_length = 30\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Define the dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "dataset = CharDataset(sequences, targets)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "362671d1-4314-48ec-b464-80f77c1f104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, test_loader, device, num_epochs=10, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "        val_losses.append(val_loss / len(test_loader))\n",
    "        val_accuracies.append(100 * correct / total)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}, Val Accuracy: {val_accuracies[-1]}%')\n",
    "\n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    print(f'Training time: {training_time} seconds')\n",
    "    return train_losses, val_losses, val_accuracies, training_time, sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97e6e72c-c1a6-4533-b433-2b6cc6776f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10, Train Loss: 1.6987307694918523, Val Loss: 1.5201707640668132, Val Accuracy: 53.98322522223667%\n",
      "Epoch 2/10, Train Loss: 1.4741471164522675, Val Loss: 1.4466245311878223, Val Accuracy: 55.59749499042914%\n",
      "Epoch 3/10, Train Loss: 1.4144343402337152, Val Loss: 1.420986441244562, Val Accuracy: 56.36406019554137%\n",
      "Epoch 4/10, Train Loss: 1.377680996993938, Val Loss: 1.3944001756245148, Val Accuracy: 57.178591761441325%\n",
      "Epoch 5/10, Train Loss: 1.3519627196277206, Val Loss: 1.3768365035368941, Val Accuracy: 57.65825536931857%\n",
      "Epoch 6/10, Train Loss: 1.3322405070835874, Val Loss: 1.3689436094554348, Val Accuracy: 57.6905317990075%\n",
      "Epoch 7/10, Train Loss: 1.3168419872592798, Val Loss: 1.3603249148124261, Val Accuracy: 58.07740067152905%\n",
      "Epoch 8/10, Train Loss: 1.3043302338628062, Val Loss: 1.3557574289732806, Val Accuracy: 58.13388442348469%\n",
      "Epoch 9/10, Train Loss: 1.2934165609871184, Val Loss: 1.350373524806996, Val Accuracy: 58.357129728833165%\n",
      "Epoch 10/10, Train Loss: 1.2836726293677376, Val Loss: 1.3475869974047714, Val Accuracy: 58.544064050781586%\n",
      "Training time: 684.5796096324921 seconds\n",
      "\n",
      "Training GRU model...\n",
      "Epoch 1/10, Train Loss: 1.6877376107019182, Val Loss: 1.5305989685923083, Val Accuracy: 53.791359779085774%\n",
      "Epoch 2/10, Train Loss: 1.492279960893039, Val Loss: 1.4737851519866436, Val Accuracy: 55.16893572955938%\n",
      "Epoch 3/10, Train Loss: 1.4463520753472547, Val Loss: 1.449965423164324, Val Accuracy: 55.96194967566671%\n",
      "Epoch 4/10, Train Loss: 1.420160255567462, Val Loss: 1.4357143559814252, Val Accuracy: 56.25736866406961%\n",
      "Epoch 5/10, Train Loss: 1.4034876254650621, Val Loss: 1.4211233493861835, Val Accuracy: 56.63078902422077%\n",
      "Epoch 6/10, Train Loss: 1.391482911204649, Val Loss: 1.4214617699914736, Val Accuracy: 56.52992518144285%\n",
      "Epoch 7/10, Train Loss: 1.3840113648534573, Val Loss: 1.418656630688403, Val Accuracy: 56.48554509062056%\n",
      "Epoch 8/10, Train Loss: 1.3782774951529106, Val Loss: 1.4154602468389925, Val Accuracy: 56.4604412008625%\n",
      "Epoch 9/10, Train Loss: 1.372910635991474, Val Loss: 1.4188441871160484, Val Accuracy: 56.756308473011075%\n",
      "Epoch 10/10, Train Loss: 1.3697792003150309, Val Loss: 1.4118625379402063, Val Accuracy: 56.84686178963837%\n",
      "Training time: 596.3202829360962 seconds\n",
      "\n",
      "Comparison of LSTM and GRU models:\n",
      "LSTM - Train Loss: 1.2836726293677376, Val Loss: 1.3475869974047714, Val Accuracy: 58.544064050781586%, Training Time: 684.5796096324921 seconds, Model Size: 559681 parameters\n",
      "GRU - Train Loss: 1.3697792003150309, Val Loss: 1.4118625379402063, Val Accuracy: 56.84686178963837%, Training Time: 596.3202829360962 seconds, Model Size: 428097 parameters\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize and train the LSTM model\n",
    "input_size = len(chars)\n",
    "hidden_size = 256\n",
    "output_size = len(chars)\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "lstm_train_losses, lstm_val_losses, lstm_val_acc, lstm_training_time, lstm_model_size = train_model(lstm_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "print(\"\\nTraining GRU model...\")\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "gru_train_losses, gru_val_losses, gru_val_acc, gru_training_time, gru_model_size = train_model(gru_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of LSTM and GRU models:\")\n",
    "print(f\"LSTM - Train Loss: {lstm_train_losses[-1]}, Val Loss: {lstm_val_losses[-1]}, Val Accuracy: {lstm_val_acc[-1]}%, Training Time: {lstm_training_time} seconds, Model Size: {lstm_model_size} parameters\")\n",
    "print(f\"GRU - Train Loss: {gru_train_losses[-1]}, Val Loss: {gru_val_losses[-1]}, Val Accuracy: {gru_val_acc[-1]}%, Training Time: {gru_training_time} seconds, Model Size: {gru_model_size} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b884d02-e6aa-4afe-8cf5-3420a6227025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10, Train Loss: 1.8350824954450917, Val Loss: 1.628255730446768, Val Accuracy: 51.17203785307949%\n",
      "Epoch 2/10, Train Loss: 1.5772494181982075, Val Loss: 1.537652634712587, Val Accuracy: 53.64656412923124%\n",
      "Epoch 3/10, Train Loss: 1.5113313931777295, Val Loss: 1.5032436849738562, Val Accuracy: 54.39788768699036%\n",
      "Epoch 4/10, Train Loss: 1.4729554098841109, Val Loss: 1.4710882837171877, Val Accuracy: 55.33973183666333%\n",
      "Epoch 5/10, Train Loss: 1.4472309622253177, Val Loss: 1.4569780490758155, Val Accuracy: 55.629771420118075%\n",
      "Epoch 6/10, Train Loss: 1.4284298277694605, Val Loss: 1.443425266344658, Val Accuracy: 55.890672560103646%\n",
      "Epoch 7/10, Train Loss: 1.413957880939209, Val Loss: 1.4319413178143525, Val Accuracy: 56.40933685385502%\n",
      "Epoch 8/10, Train Loss: 1.4023693522723735, Val Loss: 1.4265667987293849, Val Accuracy: 56.54158055883052%\n",
      "Epoch 9/10, Train Loss: 1.3926018366287716, Val Loss: 1.4163671794186634, Val Accuracy: 56.70027300480112%\n",
      "Epoch 10/10, Train Loss: 1.3836346393214924, Val Loss: 1.4139411481577162, Val Accuracy: 56.803826550053124%\n",
      "Training time: 461.5117013454437 seconds\n",
      "\n",
      "Training GRU model...\n",
      "Epoch 1/10, Train Loss: 1.7944772681814463, Val Loss: 1.6102656558951085, Val Accuracy: 51.36076531001063%\n",
      "Epoch 2/10, Train Loss: 1.5668600955128464, Val Loss: 1.5359636482259562, Val Accuracy: 53.81646366884383%\n",
      "Epoch 3/10, Train Loss: 1.5128221870391996, Val Loss: 1.5040588283839749, Val Accuracy: 54.65520255701048%\n",
      "Epoch 4/10, Train Loss: 1.4840545622750527, Val Loss: 1.4894773077157744, Val Accuracy: 54.9017586171343%\n",
      "Epoch 5/10, Train Loss: 1.464059215874694, Val Loss: 1.4702677944919806, Val Accuracy: 55.457630461777086%\n",
      "Epoch 6/10, Train Loss: 1.4507033871684483, Val Loss: 1.4674451403144577, Val Accuracy: 55.48901032397466%\n",
      "Epoch 7/10, Train Loss: 1.4407318210848021, Val Loss: 1.4551117210229512, Val Accuracy: 55.8189471607949%\n",
      "Epoch 8/10, Train Loss: 1.4326224698718772, Val Loss: 1.4506728443959904, Val Accuracy: 55.93370779968889%\n",
      "Epoch 9/10, Train Loss: 1.4253852262417612, Val Loss: 1.4474729318695374, Val Accuracy: 55.88170688519005%\n",
      "Epoch 10/10, Train Loss: 1.4204934396007864, Val Loss: 1.4471953717443495, Val Accuracy: 55.796532973510914%\n",
      "Training time: 458.2108197212219 seconds\n",
      "\n",
      "Comparison of LSTM and GRU models:\n",
      "LSTM - Train Loss: 1.3836346393214924, Val Loss: 1.4139411481577162, Val Accuracy: 56.803826550053124%, Training Time: 461.5117013454437 seconds, Model Size: 148801 parameters\n",
      "GRU - Train Loss: 1.4204934396007864, Val Loss: 1.4471953717443495, Val Accuracy: 55.796532973510914%, Training Time: 458.2108197212219 seconds, Model Size: 115777 parameters\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the LSTM model\n",
    "input_size = len(chars)\n",
    "hidden_size = 128\n",
    "output_size = len(chars)\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "lstm_train_losses, lstm_val_losses, lstm_val_acc, lstm_training_time, lstm_model_size = train_model(lstm_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "print(\"\\nTraining GRU model...\")\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "gru_train_losses, gru_val_losses, gru_val_acc, gru_training_time, gru_model_size = train_model(gru_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of LSTM and GRU models:\")\n",
    "print(f\"LSTM - Train Loss: {lstm_train_losses[-1]}, Val Loss: {lstm_val_losses[-1]}, Val Accuracy: {lstm_val_acc[-1]}%, Training Time: {lstm_training_time} seconds, Model Size: {lstm_model_size} parameters\")\n",
    "print(f\"GRU - Train Loss: {gru_train_losses[-1]}, Val Loss: {gru_val_losses[-1]}, Val Accuracy: {gru_val_acc[-1]}%, Training Time: {gru_training_time} seconds, Model Size: {gru_model_size} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6885a0ec-9212-458d-8eba-64c5e4a29d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset with sequence length 30\n",
    "sequence_length = 50\n",
    "chars = sorted(list(set(text)))\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "encoded_text = [char_to_int[ch] for ch in text]\n",
    "\n",
    "sequences = []\n",
    "targets = []\n",
    "for i in range(0, len(encoded_text) - sequence_length):\n",
    "    seq = encoded_text[i:i+sequence_length]\n",
    "    target = encoded_text[i+sequence_length]\n",
    "    sequences.append(seq)\n",
    "    targets.append(target)\n",
    "\n",
    "sequences = torch.tensor(sequences, dtype=torch.long)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "# Define the dataset class\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.sequences[index], self.targets[index]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "dataset = CharDataset(sequences, targets)\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2855ceb6-b710-4c23-a88e-69c057ff1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n",
      "Epoch 1/10, Train Loss: 1.7794938683125059, Val Loss: 1.5889111647165444, Val Accuracy: 52.078504857241484%\n",
      "Epoch 2/10, Train Loss: 1.5394333560255402, Val Loss: 1.5149517819345237, Val Accuracy: 54.289031644917046%\n",
      "Epoch 3/10, Train Loss: 1.479664434427819, Val Loss: 1.4759218515299273, Val Accuracy: 55.28199794682363%\n",
      "Epoch 4/10, Train Loss: 1.4466348021021191, Val Loss: 1.4591693799350705, Val Accuracy: 55.650942085184404%\n",
      "Epoch 5/10, Train Loss: 1.4250733407254097, Val Loss: 1.4415393042721696, Val Accuracy: 56.159753260202%\n",
      "Epoch 6/10, Train Loss: 1.4086411533525351, Val Loss: 1.430341024885763, Val Accuracy: 56.49955843259261%\n",
      "Epoch 7/10, Train Loss: 1.3963485210556807, Val Loss: 1.4218892815362019, Val Accuracy: 56.62508013215642%\n",
      "Epoch 8/10, Train Loss: 1.386787021674003, Val Loss: 1.4167328518625655, Val Accuracy: 56.84429481460893%\n",
      "Epoch 9/10, Train Loss: 1.379176461958266, Val Loss: 1.4164136524557451, Val Accuracy: 56.85505381742869%\n",
      "Epoch 10/10, Train Loss: 1.371311936812325, Val Loss: 1.4097141520565293, Val Accuracy: 57.04243978320609%\n",
      "Training time: 1022.822380065918 seconds\n",
      "\n",
      "Training GRU model...\n",
      "Epoch 1/10, Train Loss: 1.7562652281693245, Val Loss: 1.5950732282435012, Val Accuracy: 52.25423523663082%\n",
      "Epoch 2/10, Train Loss: 1.5522895941791088, Val Loss: 1.5355667805302369, Val Accuracy: 53.6663543567237%\n",
      "Epoch 3/10, Train Loss: 1.5064932947229952, Val Loss: 1.507925195661141, Val Accuracy: 54.67321770393914%\n",
      "Epoch 4/10, Train Loss: 1.4832641942705251, Val Loss: 1.4949647225802887, Val Accuracy: 54.94488252513796%\n",
      "Epoch 5/10, Train Loss: 1.4690265242962117, Val Loss: 1.4797168670887928, Val Accuracy: 55.356414382993606%\n",
      "Epoch 6/10, Train Loss: 1.4571806466095487, Val Loss: 1.474567018616015, Val Accuracy: 55.36358705154011%\n",
      "Epoch 7/10, Train Loss: 1.4496270704604584, Val Loss: 1.4728595632557477, Val Accuracy: 55.31875787312446%\n",
      "Epoch 8/10, Train Loss: 1.4452961727393236, Val Loss: 1.4714043082790134, Val Accuracy: 55.47745316471585%\n",
      "Epoch 9/10, Train Loss: 1.4414670999765635, Val Loss: 1.4608012825096741, Val Accuracy: 55.723565354217754%\n",
      "Epoch 10/10, Train Loss: 1.437269423326381, Val Loss: 1.4593640277424011, Val Accuracy: 55.81546516996983%\n",
      "Training time: 966.4055812358856 seconds\n",
      "\n",
      "Comparison of LSTM and GRU models:\n",
      "LSTM - Train Loss: 1.371311936812325, Val Loss: 1.4097141520565293, Val Accuracy: 57.04243978320609%, Training Time: 1022.822380065918 seconds, Model Size: 148801 parameters\n",
      "GRU - Train Loss: 1.437269423326381, Val Loss: 1.4593640277424011, Val Accuracy: 55.81546516996983%, Training Time: 966.4055812358856 seconds, Model Size: 115777 parameters\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize and train the LSTM model\n",
    "input_size = len(chars)\n",
    "hidden_size = 128\n",
    "output_size = len(chars)\n",
    "num_epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "print(\"Training LSTM model...\")\n",
    "lstm_model = LSTMModel(input_size, hidden_size, output_size)\n",
    "lstm_train_losses, lstm_val_losses, lstm_val_acc, lstm_training_time, lstm_model_size = train_model(lstm_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "print(\"\\nTraining GRU model...\")\n",
    "gru_model = GRUModel(input_size, hidden_size, output_size)\n",
    "gru_train_losses, gru_val_losses, gru_val_acc, gru_training_time, gru_model_size = train_model(gru_model, train_loader, test_loader, device, num_epochs, lr)\n",
    "\n",
    "# Compare results\n",
    "print(\"\\nComparison of LSTM and GRU models:\")\n",
    "print(f\"LSTM - Train Loss: {lstm_train_losses[-1]}, Val Loss: {lstm_val_losses[-1]}, Val Accuracy: {lstm_val_acc[-1]}%, Training Time: {lstm_training_time} seconds, Model Size: {lstm_model_size} parameters\")\n",
    "print(f\"GRU - Train Loss: {gru_train_losses[-1]}, Val Loss: {gru_val_losses[-1]}, Val Accuracy: {gru_val_acc[-1]}%, Training Time: {gru_training_time} seconds, Model Size: {gru_model_size} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
